{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Basic Ngram Model","metadata":{}},{"cell_type":"markdown","source":"The code in this notebook is inspired by the code from this source: https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0. I did change quite a few things from the source code, which I commented on throughout my code. Right now, I implemented the model as a trigram model, but you can implement it as any ngram model.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport string\nimport random\nimport torch\nimport torch.nn as nn\nimport random\nimport time\nfrom typing import List\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nif torch.backends.cudnn.enabled:\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/lyrics-with-sentiment/lyrics_sentiments.csv\") #in kaggle\n#df_train = pd.read_csv(\"data/lyrics.csv\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['genre'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\npop_lyrics = list()\nwhile i < len(df_train.index):\n    if df_train['genre'][i] == 'Pop' and type(df_train['lyrics'][i]) == str:\n        pop_lyrics.append(df_train['lyrics'][i])\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_multiple_strings(cur_string, replace_list):\n    for cur_word in replace_list:\n        cur_string = cur_string.replace(cur_word, '')\n    return cur_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lemma = nltk.stem.wordnet.WordNetLemmatizer()\nrempunc = '(),.:[]'\nfrom nltk.tokenize import wordpunct_tokenize\ndef clean(sentence):\n    without_some_punc = remove_multiple_strings(sentence, rempunc)\n    sentence = without_some_punc.lower()\n    sentence  = sentence.replace('\\n', ' nnnnnn ')\n    sentence = wordpunct_tokenize(sentence)\n    postagged = nltk.pos_tag(sentence)\n    replace_newline = []\n    for word in postagged:\n        w,t = word\n        if w == 'nnnnnn':\n            t = 'NEWLINE'\n        replace_word = w + '-' + t\n        replace_newline.append(replace_word)\n    return replace_newline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = 'hello my good siR, \\n mY Name Is mister Blue \\n'\nclean_ex = clean(example)\nprint(clean_ex)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that in the code below I removed the tokenize function from the original code, because I already lemmatized the sentence in the same way as we did for the neural network. I used \"start\" and \"stop\" to generated the n-grams, so a bit different from the neural network. Furthermore, I changed a few things in the word generation so it would start generating from two given words instead of from randomly initialized words. Finally, I did not split the sentences at periods, since this doesn't really make sense for songs. Instead, I split the input at every new song. ","metadata":{}},{"cell_type":"code","source":"def get_ngrams(n: int, tokens: list) -> list:\n    \"\"\"\n    :param n: n-gram size\n    :param tokens: tokenized sentence\n    :return: list of ngrams\n    ngrams of tuple form: ((previous wordS!), target word)\n    \"\"\"\n    l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n    return l\n\ndef top_k_sampling(conditional_probability, target_words, k):\n    n_conds = len(conditional_probability)\n    #conditional_probability = np.array(conditional_probability)\n    k = min(n_conds,k)\n    top_k_probabilities, top_k_indices= tf.math.top_k(conditional_probability, k=k, sorted=True)\n    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n    top_k_redistributed_probability=tf.nn.softmax(np.log(top_k_probabilities))\n    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n    sampled_index = np.random.choice(top_k_indices, p=top_k_redistributed_probability)\n    sampled_token = target_words[sampled_index]\n    return sampled_token\n\n\nclass NgramModel(object):\n\n    def __init__(self, n, k = 5):\n        self.k = k\n        self.n = n\n        # dictionary that keeps list of candidate words given context\n        self.context = {}\n        #same as above but only tracks counts:\n        self.context_count = {}\n        # keeps track of how many times ngram has appeared in the text before\n        self.ngram_counter = {}\n        #probability dict\n        self.conditional_probs = {}\n\n    def update(self, sentence: str, asstr = True, types = True) -> None:\n        \"\"\"\n        Updates Language Model\n        :param sentence: input text\n        \"\"\"\n        n = self.n\n        ngrams = get_ngrams(n, clean(sentence))\n        for ngram in ngrams:\n            if asstr:\n                if types:\n                    context = ngram[0][0].split('-')[1] + '_' + ngram[0][1].split('-')[1]\n                    target_word = ngram[1].split('-')[1]\n                    ngram = '_'.join([context,target_word])\n                else:\n                    context = '_'.join(ngram[0])\n                    target_word = ngram[1]\n                    ngram = '_'.join([context,target_word])\n            else:\n                context, target_word = ngram\n            if ngram in self.ngram_counter:\n                self.ngram_counter[ngram] += 1.0\n            else:\n                self.ngram_counter[ngram] = 1.0\n\n            \n            if context in self.context:\n                self.context[context].add(target_word)\n                self.context_count[context] += 1.0\n            else:\n                self.context[context] = set([target_word])\n                self.context_count[context] = 1.0\n\n    def prob(self, context, token):\n        \"\"\"\n        Calculates probability of a candidate token to be generated given a context\n        :return: conditional probability\n        \"\"\"\n        try:\n            count_of_token = self.ngram_counter[(context, token)]\n            count_of_context = float(len(self.context[context]))\n            result = count_of_token / count_of_context\n\n        except KeyError:\n            result = 0.0\n        return result\n\n    def random_token(self, context):\n        \"\"\"\n        Given a context we \"semi-randomly\" select the next word to append in a sequence\n        :param context:\n        :return:\n        \"\"\"\n        r = random.random()\n        map_to_probs = {}\n        token_of_interest = self.context[context]\n        for token in token_of_interest:\n            map_to_probs[token] = self.prob(context, token)\n\n        summ = 0\n        for token in sorted(map_to_probs):\n            summ += map_to_probs[token]\n            if summ > r:\n                return token\n\n    def generate_text(self, context_queue, token_count: int):\n        \"\"\"\n        :param token_count: number of words to be produced\n        :return: generated text\n        \"\"\"\n        n = self.n\n        result = []\n        for _ in range(token_count):\n            obj = self.random_token(tuple(context_queue))\n            if obj == 'NEWLINE':\n                obj = '\\n'\n            result.append(obj)\n            context_queue.pop(0)\n            context_queue.append(obj)\n        return ' '.join(result)\n    \n    def create_probabilities(self, asstr = True):\n        self.conditional_probs = {}\n        for context in self.context.keys():\n            cont_dict = {}\n            targets = list(self.context[context])\n            if asstr:\n                targ_count = np.array([self.ngram_counter['_'.join([context,target])] for target in targets])\n            else:\n                targ_count = np.array([self.ngram_counter[(context,target)] for target in targets])\n            cond_prop = targ_count / self.context_count[context]\n            cont_dict['target_words'] = targets\n            cont_dict['probabilities'] = list(cond_prop)\n            self.conditional_probs[context] = cont_dict\n            \n    def generate_text_withtopk(self, context_queue, token_count: int):\n        \"\"\"\n        :param token_count: number of words to be produced\n        :return: generated text\n        \"\"\"\n        result = []\n        for _ in range(token_count):\n            conditional_prob = self.conditional_probs[tuple(context_queue)]\n            probs = conditional_prob['probabilities']\n            words = conditional_prob['target_words']\n            obj = top_k_sampling(probs, words, self.k)\n            if obj == 'NEWLINE':\n                result.append('\\n')\n            else:\n                result.append(obj)\n            context_queue.pop(0)\n            context_queue.append(obj)\n        return ' '.join(result).replace(' \\' ', '\\'')\n\n\n\ndef create_ngram_model(n, intext, k = 5):\n    m = NgramModel(n ,k)\n    for sentence in intext:\n        m.update(sentence)\n    m.create_probabilities() \n    return m\n\nif False:\n    if __name__ == \"__main__\":\n        start = time.time()\n        m = create_ngram_model(3, pop_lyrics) \n        print (f'Language Model creating time: {time.time() - start}')\n        start = time.time()\n        random.seed(7)\n        print(f'{\"=\"*50}\\nGenerated text:')\n        print(m.generate_text_withtopk([\"i\", \"am\"], 200)) #change this if you want different input words/a different length\n        print(f'{\"=\"*50}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gendf = df_train[df_train['genre'] == 'Pop']\ngensentdf = gendf[gendf['sentiment'] == 'Negative']\nlyrics = list(gensentdf.lyrics)\nn_10 = create_ngram_model(5, lyrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs = n_10.conditional_probs\nprobs[list(probs.keys())[5]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastconds = ngram_model.conditional_probs\nex1 = list(lastconds.keys())[7]\nex2 = lastconds['chorus_]']\nprint(ex2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conditional_probs_per_input = {}\nfor genre in df_train['genre'].unique():\n    start = time.time()\n    gen_dict = {}\n    gendf = df_train[df_train['genre'] == genre]\n    for sent in df_train['sentiment'].unique():\n        gensentdf = gendf[gendf['sentiment'] == sent]\n        lyrics = list(gensentdf.lyrics)\n        print(len(lyrics))\n        ngram_model = create_ngram_model(3, lyrics)\n        gen_dict[sent] = ngram_model.conditional_probs\n        \n    conditional_probs_per_input[genre] = gen_dict\n    print ('Finished for {} in {}'.format(genre,time.time() - start))\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef generate_text(conditional_dict, context_queue, token_count: int, k = 5):\n        \"\"\"\n        :param token_count: number of words to be produced\n        :return: generated text\n        \"\"\"\n        result = [] + context_queue\n        for _ in range(token_count):\n            context_string = '_'.join(context_queue)\n            conditional_prob = conditional_dict[context_string]\n            probs = conditional_prob['probabilities']\n            words = conditional_prob['target_words']\n            obj = top_k_sampling(probs, words, k)\n            if obj == 'NEWLINE':\n                result.append('\\n')\n            else:\n                result.append(obj)\n            context_queue.pop(0)\n            context_queue.append(obj)\n        return ' '.join(result).replace(' \\' ', '\\'')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_song(all_conds,genre, sentiment, context_queue, token_count, k = 5):\n    conddict = all_conds[genre][sentiment]\n    return generate_text(conddict, context_queue, token_count, k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(create_song(conditional_probs_per_input, 'Rock', 'Positive',['you', 'want'], 300, 15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('ngrams_probabilities.json', 'w') as outfile:\n    json.dump(conditional_probs_per_input, outfile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n    random.seed(7)\n    print(f'{\"=\"*50}\\nGenerated text:')\n    print(m.generate_text((\"i\", \"am\"), 30)) #change this if you want different input words/a different length\n    print(f'{\"=\"*50}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_probabilities(context_dict, context_counter_dict, ngrams):\n    conditional_probabilities = {}\n    for context in context_dict.keys():\n        cont_dict = {}\n        targets = list(context_dict[context])\n        targ_count = np.array([ngrams[(context,target)] for target in targets])\n        cond_prop = targ_count / context_counter_dict[context]\n        cont_dict['target_words'] = targets\n        cont_dict['probabilities'] = cond_prop\n        conditional_probabilities[context] = cont_dict\n    return conditional_probabilities\n\nstart = time.time()\ncondprobs = return_probabilities(m.context, m.context_count,m.ngram_counter)\nprint (f'Conditional Probabilities Creation Time: {time.time() - start}')\nprint(condprobs[list(condprobs.keys())[512]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}